
== todo

- messenger lookup() and failure() upcalls

- how to get usage feedback to monitor?

- change messenger entity_inst_t
 - no more rank!  make it a uniquish nonce?



monitor
?- monitor user lib that handles resending, redirection of mon requests.
- elector
/- organize monitor store

osdmon
- distribute
- recovery: store elector epochs with maps..
- monitor needs to monitor some osds...
- monitor pgs, notify on out
- watch osd utilization; adjust overload in cluster map

mdsmon

osd/rados
- flag missing log entries on crash recovery  --> WRNOOP? or WRLOST?
- consider implications of nvram writeahead logs
- fix heartbeat wrt new replication
- mark residual pgs obsolete  ???
- rdlocks
- optimize remove wrt recovery pushes
- pg_bit changes
- report crashed pgs?

messenger
/- share same tcp socket for sender and receiver
/- graceful connection teardown
- close idle connections
- generalize out a transport layer?  
  - eg reliable tcp for most things, connectionless unreliable datagrams for monitors?
  - or, aggressive connection closing on monitors?  or just max_connections and an lru?
- osds: forget idle client addrs

objecter

objectcacher
- ocacher caps transitions vs locks
- test read locks

reliability
- heartbeat vs ping
- osdmonitor, filter

ebofs
- fix sync() 
- clone()
- snapshots
- combine inodes and/or cnodes into same blocks
- allow btree sets instead of maps
- nonblocking write on missing onodes?
- verify LRU behavior sensible: writes go to mid, not top!
- fix bug in node rotation on insert (and reenable)
- fix NEAR_LAST_FWD   (?)
- journaling?  in NVRAM?



bugs/stability
- figure out weird 40ms latency with double log entries


general
- timer needs cancel sets, schedulers need to cancel outstanding events on shutdown
  - well, just figure out general timer cancellation strategy that avoids races


remaining hard problems
- how to cope with file size changes and read/write sharing
- mds failure recovery (of course)


crush
- more efficient failure when all/too many osds are down

mds
- distributed client management
- anchormgr
  - 2pc
  - independent journal
  - distributed?
- link count management
  - also 2pc
- chdir (directory opens!)
- rewrite logstream
  - clean up
  - be smart about rados ack vs reread
  - log locking?  root log object
  - trimming, rotation

- efficient stat for single writers
- lstat vs stat
- add FILE_CAP_EXTEND capability bit
- only share osdmap updates with clients holding capabilities
- delayed replica caps release... we need to set a timer event? (and cancel it when appropriate?)
- finish hard links!
 - reclaim danglers from inode file on discover...
 - fix rename wrt hard links
- interactive hash/unhash interface
- test hashed readdir
- make logstream.flush align itself to stripes

- carefully define/document frozen wrt dir_auth vs hashing



client
- mixed lazy and non-lazy io will clobber each others' caps in the buffer cache

- test client caps with meta exports
- some heuristic behavior to consolidate caps to inode auth
- client will re-tx anything it needed to say upon rx of new mds notification (?)






MDS TODO
- fix hashed readdir: should (optionally) do a lock on dir namespace?
- fix hard links
  - they mostly work, but they're fragile
- sync clients on stat
  - will need to ditch 10s client metadata caching before this is useful
  - implement truncate
- implement hashed directories
- statfs?
- rewrite journal + recovery
- figure out online failure recovery
- more distributed fh management?
- btree directories (for efficient large directories)
- consistency points/snapshots

- fix MExportAck and others to use dir+dentry, not inode
  (otherwise this all breaks with hard links.. altho it probably needs reworking already?)





why qsync could be wrong (for very strict POSIX) : varying mds -> client message transit or processing times.
- mds -> 1,2 : qsync
- client1 writes at byte 100
- client1 -> mds : qsync reply (size=100)
- client1 writes at byte 300
- client1 -> client2 (outside channel)
- client2 writes at byte 200
- client2 -> mds : qsync reply (size=200)
-> stat results in size 200, even though at no single point in time was the max size 500.
-> for correct result, need to _stop_ client writers while gathering metadata.


SAGE:

- string table?

- hard links
 - fix MExportAck and others to use dir+dentry, not inode
   (otherwise this all breaks with hard links.. altho it probably needs reworking already!)

- do real permission checks?



CLIENT TODO

- statfs





ISSUES


- discover
 - soft: authority selectively repicates, or sets a 'forward' flag in reply
 - hard: authority always replicates (eg. discover for export)
 - forward flag (see soft)
 - error flag   (if file not found, etc.)
 - [what was i talking about?] make sure waiters are properly triggered, either upon dir_rep update, or (empty!) discover reply



DOCUMENT
- cache, distributed cache structure and invariants
- export process
- hash/unhash process


TEST
- hashing
 - test hash/unhash operation
 - hash+export: encode list of replicated dir inodes so they can be discovered before import is procesed.
 - test nauthitems (wrt hashing?)


IMPLEMENT

- smarter balancing
  - popularity calculation and management is inconsistent/wrong.
  - does it work?

- dump active config in run output somewhere


